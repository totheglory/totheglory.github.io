<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>terrence mu&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-02-10T03:55:19.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>terrence mu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JAVA IO -- File</title>
    <link href="http://yoursite.com/2017/02/10/file/"/>
    <id>http://yoursite.com/2017/02/10/file/</id>
    <published>2017-02-10T03:54:15.000Z</published>
    <updated>2017-02-10T03:55:19.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="JAVA IO, JDK , File" scheme="http://yoursite.com/tags/JAVA-IO-JDK-File/"/>
    
  </entry>
  
  <entry>
    <title>zab 协议</title>
    <link href="http://yoursite.com/2016/12/29/zab/"/>
    <id>http://yoursite.com/2016/12/29/zab/</id>
    <published>2016-12-29T01:27:47.000Z</published>
    <updated>2017-02-03T10:03:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Zab 协议</strong><br>ZAB协议是专门为ZooKeeper设计的崩溃可恢复的原子消息广播算法。ZooKeeper的分布式一致性主要由ZAB协议来实现。它通过一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器的状态变更以事务Proposal的形式广播到所有的副本进程上去。<br>协议的核心是所有事务请求必须由一个全局唯一的服务器来协调处理，即leader，而余下的其他服务器则为follower。leader服务器负责将一个客户端事务请求转换为一个事务Proposal提议，并将该Proposal分发给集群中所有的Follower服务器。如果Leader接收到了超过半数的Follower服务器进行了正确的反馈，leader就会再次向Follower发送Commit消息，要求其将前一个Proposal提交。</p>
<p>ZAB协议包含两个重要部分:崩溃恢复和消息广播。</p>
<p>一个由3台机器组成的ZAB服务，通常由一个leader，2个follower服务器组成。某个时间加入一个Follower服务器挂了,整个ZAB集群是不会中断服务的，因为leader服务器仍然可以获得过半的机器包括自身的支持。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Zab 协议&lt;/strong&gt;&lt;br&gt;ZAB协议是专门为ZooKeeper设计的崩溃可恢复的原子消息广播算法。ZooKeeper的分布式一致性主要由ZAB协议来实现。它通过一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器
    
    </summary>
    
    
      <category term="zab, ZooKeeper" scheme="http://yoursite.com/tags/zab-ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper简介</title>
    <link href="http://yoursite.com/2016/12/25/zookeeper/"/>
    <id>http://yoursite.com/2016/12/25/zookeeper/</id>
    <published>2016-12-25T07:34:57.000Z</published>
    <updated>2016-12-29T11:07:45.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>zookeeper简介</strong><br>zookeeper为分布式应用提供了高效可靠的基础服务，它并没有直接实用paxos算法作为分布式一致性上的解决方法，而是采用了ZAB协议(ZooKeeper Atomic Broadcast)。<br>它提供了如下特性：<br>1.顺序一致性，即从同一个客户端发起的事务请求，最终将会严格地按照其发送顺序被应用到ZooKeeper中。<br>2.原子性，所有事务请求最终处理的结果在整个集群是一致的，不存在一部分执行了该事务，一部分没有的情况。<br>3.单一视图，无论客户端连接的是哪台zookeeper服务器，其看到的服务端数据模型都是一致的。<br>4.可靠性，一旦服务端成功应用了一条事务，并完成对客户端的响应，那么该事务所引起的服务端状态的变更会一直保持下去，直到一个新的事务对其做了改变。<br>5.实时性，zookeeper能够保证在一定的时间段内，客户端最终一定能从服务端读到最新状态的数据。</p>
<p>zookeeper的设计目标:<br>1.简单数据模型: zookeeper的树型命名空间由一系列ZNode结点组成，有点类似于文件系统的层级结构。<br>2.方便构建集群: 3-5台机器即可组成一个集群。每台机器都会在内存中维护当前的服务器状态，且每台服务器之间保持互相通信。<br>3.支持顺序访问。<br>4.高性能。全部数据存储在内存中，尤其适用于读操作的场景。</p>
<p>zookeeper集群<br>它并非传统的master/slave式的集群模式，而是采用了leader，follower和observer三种角色。<br>zookeeper中包含一台选举出来的leader，它提供客户端的读写服务。<br>其他机器为Follower和observer，他们只提供读服务。且二者之间的区别在于，follower除了提供读服务外，还参与leader的选举，以及写操作的”过半写成功”策略。</p>
<p>session会话<br>客户端于服务器建立第一次连接开始，客户端的会话生命周期也同事开始。通过这个长连接，客户端可以通过心跳机制来于服务端保持会话的有效性。如果客户端因为服务器压力过大，或网络故障等等造成的断开，只要在sessionTimeOut之前重新于集群建立连接，都认为会话仍有效。</p>
<p>数据节点ZNode<br>即为数据模型中的数据单元,zookeeper的所有数据都存储在一个树型结构的内存模型中。比如/path/foo1就是一个znode，每个znode不仅存储数据，也存储一系列属性信息。ZNode有永久结点和临时结点的区分。临时结点与客户端会话相关。<br>znode的版本，zookeeper会为每个znode维护一个stat，包含version znode的当前版本,cversion znode的子节点版本和aversion znode的ACL版本</p>
<p>watcher监听器<br>zookeeper允许用户在指定节点注册时间监听器。zookeeper会在事件触发后发送给感兴趣的用户。</p>
<p>ACL权限控制<br>1.CREATE 创建子节点的权限。<br>2.READ 获取节点数据和子节点列表的权限。<br>3.WRITE 更新节点数据的权限<br>4.DELETE 删除子节点的权限<br>5.ADMIN 设置节点ACL的权限</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;zookeeper简介&lt;/strong&gt;&lt;br&gt;zookeeper为分布式应用提供了高效可靠的基础服务，它并没有直接实用paxos算法作为分布式一致性上的解决方法，而是采用了ZAB协议(ZooKeeper Atomic Broadcast)。&lt;br&gt;它提供了
    
    </summary>
    
    
      <category term="分布式一致性,zookeeper,ZAB" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7-zookeeper-ZAB/"/>
    
  </entry>
  
  <entry>
    <title>paxos在chubby的应用</title>
    <link href="http://yoursite.com/2016/12/12/paxosinchubby/"/>
    <id>http://yoursite.com/2016/12/12/paxosinchubby/</id>
    <published>2016-12-12T11:49:32.000Z</published>
    <updated>2016-12-25T07:32:34.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>paxos在chubby中的应用</strong></p>
<p>paxos在chubby中主要作为日志层一致性的保证，为上层的分布式锁和数据库提供保障。<br><img src="/2016/12/12/paxosinchubby/chubby.jpg" title="chubby"></p>
<p>paxos算法的作用就在于保证chubby集群内各个副本节点的日志能够保持一致。<br>chubby事务日志中的每个value对应的Paxos算法中的一个instance。在整个chubby运行过程中，会存在多个Paxos instance。chubby会为每个Paxos instance按序分配一个全局唯一的Instance编号，并顺序写入tvirsConfigMapper到事务日志中去。在多模式下，为了提升算法执行的性能，就必须选举出一个副本节点作为paxos算法的主节点，以避免多个Paxos Round并存的情况。<br>具体地：prepare-&gt;promise-&gt;propose-&gt;accept</p>
<ol>
<li>当前至多存在k个未达成一致的Instance，将这些未决的instance各自最后接受的提案值(若尚未接受任何值,则用null来代替。)作为promise消息返回。</li>
<li>判断N是否大于当前Acceptor的最大提案编号，如果大于该值，则标记所有未决instance和所有未来的instance的最大提案编号为n，这样未决的和所有未来的instance都不能接受提案编号小于n的提案。<br>在master稳定的情况下，只需要使用一个相同的编号来执行每个instance的promise-&gt;accept模式，一旦在单个的instance中接收到了多数派的Accept反馈，即可将对应的提案值写入本地事务日志并广播commit消息告诉集群中的其他副本节点，以便于其他副本节点写入。如果某台宕机，可以主动向其他副本节点进行查询。</li>
</ol>
<p><strong>chubby的数据库层</strong></p>
<p>数据快照+事务日志，定期的数据快照避免日志过大，造成的宕机恢复时间较长。如果磁盘损坏，只能从其他副本节点索取全量的状态数据了。另外如果磁盘未损坏，宕机瞬间丢失的日志数据也可以通过其他副本节点做恢复。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;paxos在chubby中的应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;paxos在chubby中主要作为日志层一致性的保证，为上层的分布式锁和数据库提供保障。&lt;br&gt;&lt;img src=&quot;/2016/12/12/paxosinchubby/chubby.jpg&quot; 
    
    </summary>
    
    
      <category term="chubby paxos" scheme="http://yoursite.com/tags/chubby-paxos/"/>
    
  </entry>
  
  <entry>
    <title>chubby介绍</title>
    <link href="http://yoursite.com/2016/10/22/chubby/"/>
    <id>http://yoursite.com/2016/10/22/chubby/</id>
    <published>2016-10-22T05:52:42.000Z</published>
    <updated>2017-03-15T15:47:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>chubby</strong></p>
<hr>
<p><strong>chubby系统架构</strong></p>
<p>大致如下图所示：<br><img src="/source/_posts/chubby/chbbyserver.png" alt="image"><br> 一个由chubby cell组成的chubby server集群,客户端的chubby lib通过Rpc协议与服务端进行通讯，chubby cell之间通过paxos算法选举server中的master。</p>
<p>目录与文件<br>chubby的数据结构可以看作是一个由文件和目录组成的树。/chubby节点的公共前缀/chubby集群的名字/业务的相对路径, Chubby服务器内部可以解析并定位到数据节点。</p>
<p>chubby 包括acl元信息，及四个单调递增的64位编码。<br>实例编码，文件内容编码，锁编码，ACL编码<br>消息乱序或延迟造成锁失效。如一个客户端c1获取到了互斥锁L，并且在锁L的保护下发出的请求R，但请求R迟迟没有到达服务器，这时应用程序会认为该客户端进程已经失败，于是便会为另一个客户端C2分配了锁L，然后再重新发送请求R，并且成功的应用到了服务器上，此时，不幸的事情发生了，客户端C1的请求R在经过一波三折之后也到达了服务端，</p>
<p>chubby中任意一个数据节点都可以充当一个读写锁来使用，一种是单个客户端以排他(写)模式持有这个锁，另一种则是任意数目的客户端以共享(读)模式持有这个锁。<br>chubby舍弃了严格的强制锁，客户端可以在没有获取任何锁的情况下访问chubby的文件。</p>
<p>chubby中解决消息延迟和重排序引起的分布式锁问题，采用锁延迟和锁序列器两种策略。chubby 客户端以正常的方式主动释放一个锁，那么chubby服务端将会允许其他客户端能够立即获取到该锁。<br>如果是因为客户端异常而被释放的话，那么chubby服务器会为该锁保留一定的时间，称之为锁延迟。<br>锁序列器,该策略需要chubby的上层应用配合在代码中加入相应的修改逻辑。</p>
<p><strong>chubby中的事件通知机制</strong><br>客户端向服务端注册事件通知，服务端根据事件通知来通知对应的客户端。</p>
<p><strong>chubby客户端的缓存策略</strong><br>chubby客户端缓存一致性问题的处理，通过租期机制来保证缓存一致性。chubby缓存的生命周期和master租期机制紧密相关，master会维护每个客户端的数据缓存情况，并通过向客户端发送过期信息的方式来保证客户端数据的一致性。这样，chubby就能保证客户端要么能从缓存中访问到一致的数据，要么访问出错。每个客户端的缓存都有一个租期，一旦该租期到期，客户端就需要向服务端续订租期以继续维持缓存的有效性，当文件数据或元数据修改时，chubby会首先阻塞该修改操作，由master向所有缓存了该缓存的客户端发送缓存过期信号，以使其缓存过期。master在接收到所有客户端针对该缓存的应答后，再进行修改操作。</p>
<p>chubby服务端处理客户端的保持连接的请求，收到后会先阻塞等待该客户端的租期即将过期时，才会对它的租期进行续租，续租后向客户端返回保持连接请求的应答。<br>chubby的租期会话时间默认为12s，但会根据具体负载进行调节。<br>客户端收到保持连接的响应后，会立即向服务端发一个保持连接的请求，服务端再次阻塞。</p>
<hr>
<p><strong>chubby master选举后的基本流程</strong><br>1.master选举后，会确定master周期，master周期确定后，如果再收到客户端携带的其他master的编号的请求，则会拒绝。并告知客户端更新master的编号。master只要发生重新选取，即需要重新确定周期。<br>2.选举产生的新的master会立即对寻址的客户端请求进行响应，但不会立即对客户端的会话请求进行处理。<br>3.master会根据本地存储的会话和锁信息，来构建服务器内存状态。<br>4.master处理客户端的keepalive请求。<br>5.master向所有客户端发送一个故障切换的请求，等待所有的客户端接收到侯清空本地缓存，警告上层缓存失效。并应答回master。<br>6.master开始处理会话级的请求。如果客户端调用了一个故障切换前创建的句柄，当前的master会重新创建合格内存对象。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;chubby&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;chubby系统架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大致如下图所示：&lt;br&gt;&lt;img src=&quot;/source/_posts/chubby/chbbyserver.png&quot; alt=&quot;
    
    </summary>
    
    
      <category term="分布式锁,GFS,Big Table" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-GFS-Big-Table/"/>
    
  </entry>
  
  <entry>
    <title>paxos 算法</title>
    <link href="http://yoursite.com/2016/10/17/paxos/"/>
    <id>http://yoursite.com/2016/10/17/paxos/</id>
    <published>2016-10-16T17:15:01.000Z</published>
    <updated>2016-12-24T10:06:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>分布式一致性的背景</strong><br>1.面临的问题<br>通讯异常，网络分区，三态（成功，失败，超时），节点故障<br>2.相关的概念<br>(1) ACID<br>原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）<br>(2) CAP原理<br>指的是 一致性(CONSISTENCY)可用性(AVAILABILITY)分区容忍性(PARTITION TOLERANCE)这三个要素最多只能同时实现两点，不可能三者兼顾。在进行分布式架构设计时，必须做出取舍。而对于分布式数据系统，分区容忍性是基本要求，否则就失去了价值。<br>一般情况下分布式数据系统，就是在一致性和可用性之间取一个平衡。<br>对于大多数WEB应用，其实并不需要强一致性，因此牺牲一致性而换取高可用性，是多数分布式产品的方向。<br>(3) BASE理论<br>BASE理论是最终一致性的理论支撑，它的全称是 Basically Available（基本可用），Soft-state（软状态/柔性事务)，Eventually Consistent（最终一致性）。在理论逻辑上它是相反于ACID模型的，它牺牲高一致性，获得可用性和分区容忍性。</p>
<p>一致性模型的类型：<br>(1) Weak 弱一致性：当你写入一个新值后，读操作在数据副本上可能读出来，也可能读不出来。比如：网络游戏其它玩家的数据，VOIP系统等。<br>(2) Eventually 最终一致性：当你写入一个新值后，有可能读不出来，但在某个时间窗口之后保证最终能读出来。比如：DNS，电子邮件、Amazon S3，Google搜索引擎等。<br>(3) Strong 强一致性：新的数据一旦写入，在任意副本任意时刻都能读到新值。比如：文件系统，RDBMS，Azure Table等。</p>
<p>分布式算法<br>1.PC二段提交<br>第一阶段：<br>协调者向所有参与者发送”是否可以提交”–&gt; 参与者开始做执行准备:资源上锁,预留资源，写入undo/redo日志(便于回滚),准备成功则回应”可以提交”，否则回应”拒绝”<br>第二阶段：<br>所有参与者回应“可以提交” –&gt; 协调者向所有的参与者发送“正式提交”命令 –&gt; 参与者完成正式提交,释放资源，回应”完成”  –&gt; 协调者收集到所有”提交完成”，结束事务。<br>任意一个参与者回应”拒绝提交” –&gt; 协调者向所有的参与者发送“回滚”命令 –&gt; 参与者释放所有资源，回应”回滚完成”–&gt; 协调者收集到所有”回滚完成”，取消整个事务。</p>
<p>优势:原理实现简单<br>劣势:<br>单点问题–&gt;协调者<br>同步阻塞问题<br>脑裂问题–&gt;协调者区域网络断开，导致双主。<br>过于保守–&gt;所有参与者都要响应(如:参与者超时返回,参与者响应后协调者错误导致参与者所处的状态等)。</p>
<p>2.三阶段提交<br>(1) 阶段1：事务询问<br>协调者发送一个包含事务的can commit请求,询问是否可以提交操作—&gt;参与者向协调者反馈事务询问的响应</p>
<p>(2) 阶段2：precommit<br>执行事务预先提交：<br>协调者向所有参与者发送precommit请求 –&gt; 参与者收到precommit请求后，执行事务操作，写undo，redo信息。–&gt; 所有参与者成功执行事务操作,反馈ACK响应给协调者,并等待下一步指令:commit 或者 abort。</p>
<p>中断事务：<br>任何一个参与者ACK了No或者等待超时后,仍有参与者未反馈响应,则中断事务。<br>协调者向所有参与者发送中断请求 –&gt; 参与者中断事务(参与者无论是否收到协调者的abort请求或等待协调者过程中发生超时,参与者都会中断事务)</p>
<p>(3) docommit<br>执行提交<br>协调者向所有参与者发送提交请求 –&gt; 参与者提交事务,释放事务执行期间所占用的资源,完成后向协调者发送ACK消息,反馈事务提交的结果 –&gt; 协调者接收到所有参与者的ack后完成事务。</p>
<p>中断事务<br>协调者收到任一个参与者的No ACK,或者因为网络故障造成的响应超时, 协调者发送中断请求–&gt; 参与者事务回滚，释放执行期间的资源占用,反馈事务的提交结果 –&gt; 协调者收到所有反馈后,中断事务 </p>
<p>优缺点：<br>优点：对比2阶段提交，3阶段提交降低了参与者的阻塞范围, 在出现单点故障后继续达成一致。<br>缺点：参与者收到precommit消息后，如果和协调者无法网络通讯，会提交事务，导致数据不一致。</p>
<p>拜占庭将军问题 (Byzantine Generals Problem)</p>
<p>拜占庭将军问题的大致描述如下:<br>两支军队，各有一个将军，他们准备协同攻击同样的一座城市，唯一的通信方式就是派各自的信使来往于山谷。但存在会被抓获的危险。<br>当且仅当两支军队同时进攻这座城市才会成功。<br>这种情形下两支军队需要进行沟通，来确定一个进攻时间。<br>这将导致一个问题：<br>试图通过建立在一个不可靠的连接上的交流，来协同一项行动的隐患和设计上的巨大挑战。</p>
<p><strong>Paxos算法</strong></p>
<p>Paxos算法是基于消息传递,具有高度容错特性的一致性算法，是当前解决分布式一致性问题最有效的算法之一。<br>该算法解决的问题就是如何在一个可能发生诸如机器宕机或网络异常等情况的分布式系统中，快速且正确的在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常都不会破坏整个系统的一致性。<br><br>paxos的三个角色Proposer,Acceptor,learner.<br><br>paxos算法大致分为两部分，提出提案和提案获取.<br><br>1.提出提案: 分为prepare阶段和accept阶段<br>prepare阶段<br>Proposer选定出一个编号为N的提案,然后向acceptor的某个超过半数的子集成员发送编号为N的prepare请求。此时如果acceptor收到该编号为N的prepare请求,如果编号N大于它所有当前已经响应的请求编号，那么它就会将编号为N的提案作为响应反馈给Proposer，并且承诺不会再批准任何小于编号N的提案。<br>accept阶段<br>如果Proposer收到来自半数以上的Acceptor反馈的，编号N的Prepare请求响应，那么它就会发送一个针对N的值为Vn的Accept请求给Acceptor，这个Vn就是提案编号N的value值。如果acceptor收到这个针对N编号值为Vn的提案，且它尚未对大于编号N的Prepare请求作出响应，它就可以通过该提案。<br><br>注意：为保证防止死循环,proposer选取一个主proposer。<br><br>2.提案的获取：<br>acceptor通过后通知learner集合，learner集合再负责通知其余的learner。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;分布式一致性的背景&lt;/strong&gt;&lt;br&gt;1.面临的问题&lt;br&gt;通讯异常，网络分区，三态（成功，失败，超时），节点故障&lt;br&gt;2.相关的概念&lt;br&gt;(1) ACID&lt;br&gt;原子性（Atomicity）、一致性（Consistency）、隔离性（Isolat
    
    </summary>
    
    
      <category term="paxos,分布式" scheme="http://yoursite.com/tags/paxos-%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
</feed>
